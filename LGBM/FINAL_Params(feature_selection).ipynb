{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FINAL_Params(feature_selection).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLGN3Uo0KgsL","outputId":"b0443113-5db6-4770-86af-f6cbeb6e6979"},"source":["!git clone --recursive https://github.com/Microsoft/LightGBM\r\n","! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fatal: destination path 'LightGBM' already exists and is not an empty directory.\n","-- The C compiler identification is GNU 7.5.0\n","-- The CXX compiler identification is GNU 7.5.0\n","-- Check for working C compiler: /usr/bin/cc\n","-- Check for working C compiler: /usr/bin/cc -- works\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Check for working CXX compiler: /usr/bin/c++\n","-- Check for working CXX compiler: /usr/bin/c++ -- works\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n","-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n","-- Found OpenMP: TRUE (found version \"4.5\")  \n","-- Looking for CL_VERSION_2_2\n","-- Looking for CL_VERSION_2_2 - found\n","-- Found OpenCL: /usr/lib/x86_64-linux-gnu/libOpenCL.so (found version \"2.2\") \n","-- OpenCL include directory: /usr/include\n","-- Boost version: 1.65.1\n","-- Found the following Boost libraries:\n","--   filesystem\n","--   system\n","-- Performing Test MM_PREFETCH\n","-- Performing Test MM_PREFETCH - Success\n","-- Using _mm_prefetch\n","-- Performing Test MM_MALLOC\n","-- Performing Test MM_MALLOC - Success\n","-- Using _mm_malloc\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/LightGBM/build\n","\u001b[35m\u001b[1mScanning dependencies of target lightgbm\u001b[0m\n","\u001b[35m\u001b[1mScanning dependencies of target _lightgbm\u001b[0m\n","[  1%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\u001b[0m\n","[  2%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\u001b[0m\n","[  4%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n","[  5%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n","[  7%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n","[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n","[ 11%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n","[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n","[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n","[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config.cpp.o\u001b[0m\n","[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n","[ 18%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n","[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n","[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n","[ 23%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n","[ 24%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n","[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n","[ 27%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n","[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n","[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/metadata.cpp.o\u001b[0m\n","[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/parser.cpp.o\u001b[0m\n","[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/train_share_states.cpp.o\u001b[0m\n","[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/tree.cpp.o\u001b[0m\n","[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n","[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/metric.cpp.o\u001b[0m\n","[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/ifaddrs_patch.cpp.o\u001b[0m\n","[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linker_topo.cpp.o\u001b[0m\n","[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n","[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_socket.cpp.o\u001b[0m\n","[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/network.cpp.o\u001b[0m\n","[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/objective/objective_function.cpp.o\u001b[0m\n","[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/cuda_tree_learner.cpp.o\u001b[0m\n","[ 49%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n","[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n","[ 52%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config.cpp.o\u001b[0m\n","[ 53%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n","[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n","[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/linear_tree_learner.cpp.o\u001b[0m\n","[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n","[ 59%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n","[ 60%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n","[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n","[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n","[ 65%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n","[ 66%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tanM9U9j7xax"},"source":["import pandas as pd\r\n","import numpy as np\r\n","import seaborn as sns\r\n","import matplotlib.pyplot as plt\r\n","from pandas import DataFrame, concat\r\n","!pip install import_ipynb\r\n","import import_ipynb\r\n","from google.colab import drive\r\n","drive.mount('/content/drive')\r\n","from datetime import date\r\n","import lightgbm as lgbm\r\n","from lightgbm import LGBMRegressor\r\n","import time\r\n","import statsmodels.api as sm\r\n","from sklearn import linear_model, metrics\r\n","from sklearn.model_selection import train_test_split\r\n","from tqdm import tnrange, tqdm_notebook\r\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\r\n","from statsmodels.regression.quantile_regression import QuantReg\r\n","from sklearn.ensemble import GradientBoostingRegressor\r\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4SzsiaLqCrZO"},"source":["import math\r\n","from math import radians\r\n","from sklearn.preprocessing import StandardScaler\r\n","from sklearn.preprocessing import MinMaxScaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pVYbSp4X7_Qq"},"source":["#### 파라미터 ####\r\n","\r\n","# 하루의 틱\r\n","ticks = 48\r\n","\r\n","# 예측에 사용할 일수\r\n","days = 3                # 이 부분을 바꿀 수 있다.\r\n","n_days = ticks*days\r\n","\r\n","# 미래 예측할 일수\r\n","future_days = 2\r\n","future_window = ticks * future_days\r\n","\r\n","### 모든변수\r\n","# ['Hour', 'Minute', 'Day', 'WS', 'Time', 'DHI','DNI','RH','T','TARGET']\r\n","\r\n","# 사용할 변수\r\n","what_to_left = ['Time', 'WS', 'DHI','DNI','RH','T','TARGET', 'Day']\r\n","\r\n","n_features = len(what_to_left)\r\n","n_obs = n_days * n_features # 7일 예측 기준으로 7(일 수) * 48(틱 수) * 6(변수 개수)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppzWBSoP8AYz"},"source":["# 데이터 불러오기 \r\n","\r\n","raw_data = pd.read_csv('/content/drive/MyDrive/Jupyter/unlimited_power/raw_data/train/train.csv')\r\n","submission = pd.read_csv('/content/drive/MyDrive/Jupyter/unlimited_power/raw_data/sample_submission.csv')\r\n","submission.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IElz-CCj8CfJ"},"source":["def drop_clms(dataset):\r\n","  dataset['Time'] = dataset['Hour'] + dataset['Minute']*(0.5/30)\r\n","  dataset['Date'] = dataset['Day']%365\r\n","  dataset.drop('Day', axis=1, inplace=True)\r\n","  \r\n","  temp = list()\r\n","  for i in range(0, len(dataset), 48):\r\n","    temp += [24-(list(dataset.DHI[i:i+48]).count(0)*0.5)]*48 \r\n","\r\n","  dataset[\"SH\"] = temp\r\n","\r\n","  return dataset\r\n","\r\n","def cos_time(dataset):\r\n","  dataset['sin_time'] = np.sin(2*np.pi*dataset.Time/24)\r\n","  dataset['cos_time'] = np.cos(2*np.pi*dataset.Time/24)\r\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5MqRQyG8Ssv"},"source":["def get_yday(when):\r\n","  HP = date(2020,1,1)\r\n","  results = (when-HP).days\r\n","  return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CmXuz7BE8TxK"},"source":["def 절기24(Date):\r\n","  target = 0\r\n","  for i in list24:\r\n","    if Date < i:\r\n","      target = list24.index(i) - 1\r\n","      break\r\n","  if Date < 5:\r\n","    target = 23\r\n","  return target"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnURqcW48V8S"},"source":["def train_to_supervised(train, target, n_in):\r\n","\r\n","    clmns = list(train.columns)\r\n","\r\n","    # 기타 칼럼은 전과 같이 들어갈 것.\r\n","    scaled_lst = clmns\r\n","\r\n","    scaled_df = train[scaled_lst]\r\n","    target_df = target\r\n","\r\n","    # 미래 몇 번째 항목을 가져올 것인가\r\n","    future = [48, 96]\r\n","\r\n","    ### 만약에 스케일링을 하고 싶다면 ###\r\n","    # scaled_df 데이터 프레임만 스케일링 하고, 절기랑 TARGET 데이터는 그냥 두면 된다.\r\n","\r\n","    # 스케일링 해도 되고, 안해도 되는 기존에 썻던 변수들 전처리\r\n","    cols, names = list(), list()\r\n","    n_vars = 1 if type(scaled_df) is list else scaled_df.shape[1]\r\n","    n_vars2 = 1 if type(target_df) is list else target_df.shape[1]\r\n","    for i in range(n_in, 0, -1):\r\n","        cols.append(scaled_df.shift(i))\r\n","        names += [('%s(t-%d)' % (j, i)) for j in scaled_df.columns]\r\n","\r\n","    # 48과 96 후의 타겟 데이터 2개 붙이기.\r\n","    # forecast sequence (t, t+1, ... t+n)\r\n","    for i in future:\r\n","        cols.append(target_df.shift(-i))\r\n","        if i == 0:\r\n","            names += [('TARGET%d(t)' % (j+1)) for j in range(n_vars2)]\r\n","        else:\r\n","            names += [('TARGET%d(t+%d)' % (j+1, i)) for j in range(n_vars2)]\r\n","    # put it all together\r\n","    agg = concat(cols, axis=1)\r\n","    agg.columns = names\r\n","    # drop rows with NaN values\r\n","    agg.dropna(inplace=True)\r\n","    return agg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJh6MAlFAHwR"},"source":["# 머신러닝에 쓰기 위해서 재정렬 시키는 series_to_supervised 함수\r\n","\r\n","def test_to_supervised(train, n_in):\r\n","\r\n","    clmns = list(train.columns)\r\n","\r\n","    # 타켓 칼럼의 이름을 여기에 입력\r\n","    target = ['TARGET']\r\n","\r\n","    # 클래스 변수로 전환(encoding)할 칼럼을 여기에 입력\r\n","#    class_lst = ['season']\r\n","\r\n","    # 기타 칼럼은 전과 같이 들어갈 것.\r\n","#    scaled_lst = list(set(clmns) - set(class_lst))\r\n","\r\n","    scaled_df = train.copy()\r\n","#    class_df = train[class_lst]\r\n","\r\n","    ### 만약에 스케일링을 하고 싶다면 ###\r\n","    # testset의 스케일링은 구조가 상당히 까다로우므로....... 일단 나중에 하기로 함.\r\n","\r\n","    # 스케일링 해도 되고, 안해도 되는 기존에 썻던 변수들 전처리\r\n","    cols, names = list(), list()\r\n","    n_vars = 1 if type(scaled_df) is list else scaled_df.shape[1]\r\n","    for i in range(n_in, 0, -1):\r\n","        cols.append(scaled_df.shift(i))\r\n","        names += [('%s(t-%d)' % (j, i)) for j in scaled_df.columns]\r\n","\r\n","    # class라 encoding 필요한 절기 끝에 하나만 붙여놓기\r\n","#    cols.append(class_df)\r\n","#    names += class_lst\r\n","    \r\n","    # put it all together\r\n","    agg = concat(cols, axis=1)\r\n","    agg.columns = names\r\n","    # drop rows with NaN values\r\n","    agg.dropna(inplace=True)\r\n","    return agg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODgWtxW18Wlp"},"source":["소한 = get_yday(date(2020,1,6))\r\n","대한 = get_yday(date(2020,1,20))\r\n","입춘 = get_yday(date(2020,2,4))\r\n","우수 = get_yday(date(2020,2,19))\r\n","경칩 = get_yday(date(2020,3,6))\r\n","춘분 = get_yday(date(2020,3,21))\r\n","청명 = get_yday(date(2020,4,5))\r\n","곡우 = get_yday(date(2020,4,20))\r\n","입하 = get_yday(date(2020,5,6))\r\n","소만 = get_yday(date(2020,5,21))\r\n","망종 = get_yday(date(2020,6,6))\r\n","하지 = get_yday(date(2020,6,22))\r\n","소서 = get_yday(date(2020,7,7))\r\n","대서 = get_yday(date(2020,7,23))\r\n","입추 = get_yday(date(2020,8,8))\r\n","처서 = get_yday(date(2020,8,23))\r\n","백로 = get_yday(date(2020,9,8))\r\n","추분 = get_yday(date(2020,9,23))\r\n","한로 = get_yday(date(2020,10,8))\r\n","상강 = get_yday(date(2020,10,24))\r\n","입동 = get_yday(date(2020,11,8))\r\n","소설 = get_yday(date(2020,11,22))\r\n","대설 = get_yday(date(2020,12,7))\r\n","동지 = get_yday(date(2020,12,22))\r\n","\r\n","list24 = [소한, 대한, 입춘, 우수, 경칩, 춘분, 청명, 곡우, 입하, 소만, 망종, 하지, 소서, 대서, 입추, 처서, 백로, 추분, 한로, 상강, 입동, 소설, 대설, 동지]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qDBgXyL9HPR"},"source":["m = 60\r\n","\r\n","일출 = [7+47/m, 7+44/m, 7+34/m, 7+18/m, 6+56/m, 6+35/m, 6+12/m, 5+51/m, 5+33/m, 5+19/m, 5+11/m, 5+11/m, 5+17/m, 5+27/m, 5+41/m, 5+54/m, 6+8/m, 6+20/m, 6+33/m, 6+47/m, 7+3/m, 7+20/m, 7+33/m, 7+45/m]\r\n","일몰 = [17+28/m, 17+42/m, 17+58/m, 18+15/m, 18+30/m, 18+44/m, 18+58/m, 19+12/m, 19+25/m, 19+39/m, 19+50/m, 19+57/m, 19+56/m, 19+50/m, 19+35/m, 19+15/m, 18+51/m, 18+28/m, 18+6/m, 17+45/m, 17+28/m, 17+17/m, 17+13/m, 17+17/m]\r\n","경사각 = [32.92, 36.83, 40.75, 44.67, 48.58, 52.5, 56.42, 60.33, 64.25, 68.16, 72.01, 76, 72.1, 68.16, 64.25, 60.33, 56.42, 52.5, 48.58, 44.67, 40.75, 36.83, 32.92, 29]\r\n","남중 = [12+36/m, 12+41/m, 12+44/m, 12+44/m, 12+42/m, 12+38/m, 12+34/m, 12+29/m, 12+27/m, 12+27/m, 12+29/m, 12+32/m, 12+35/m, 12+37/m, 12+36/m, 12+33/m, 12+29/m, 12+23/m, 12+18/m, 12+15/m, 12+14/m, 12+16/m, 12+22/m, 12+29/m]\r\n","print(len(일출), len(일몰), len(경사각), len(남중))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"56dK3l3i-c6z"},"source":["def HRA(DHI, DNI, season, hour):\r\n","  # 위도(latitude) 기준을 일단 임의로 대전으로 설정 (위도 36.19~36.2도)\r\n","  latitude = radians(36.2)\r\n","  season = int(season)\r\n","  # 절기별 대한민국의 경사각\r\n","  tilt = radians(경사각[season])\r\n","\r\n","  # 절기별 대한민국 대전의 태양 남중시각\r\n","  hra = radians(15*(hour - 남중[season]))\r\n","\r\n","  # 구하려는 알파\r\n","  elevation = np.arcsin(np.sin(tilt) * np.sin(latitude) + np.cos(tilt) * np.cos(latitude) * np.cos(hra))\r\n","\r\n","  # 천정각(Zenith Angle)은 90 - 알파\r\n","  zenith = radians(90) - elevation\r\n","\r\n","  # GHI는 DHI + DNI * cos(천정각)\r\n","  ghi = DHI + DNI *np.cos(zenith)\r\n","\r\n","  return ghi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OYZ1qaX3H070"},"source":["def encoding(data):\r\n","  season_list = [i for i in range(0,24)]\r\n","  for k in season_list:\r\n","    data['Season_' +f'{k}'] = data['season'] == k\r\n","  return data*1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nliyo8xHM10I"},"source":["def not_minus(dataset):\r\n","  for i in range(0, len(dataset.index)):\r\n","    for j in range(0, len(dataset.columns)):\r\n","      K = dataset.iloc[i,j]\r\n","      if K < 0.09:\r\n","        dataset.iloc[i,j] = 0\r\n","  return dataset\r\n","\r\n","def not_dec(dataset):\r\n","  for i in range(0, len(dataset.index)):\r\n","    for j in range(0, len(dataset.columns) - 1):\r\n","      K = dataset.iloc[i,j]\r\n","      T = dataset.iloc[i,j+1]\r\n","      if K > 0 and T == 0:\r\n","        dataset.iloc[i,j] = 0\r\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uL3_zY8bfXtr"},"source":["def wind_and_hot(wind, hot):\r\n","  h = 743 + 176.5 * hot + 3.562 * wind - 13.14 * hot**2 -0.7466 * hot * wind - 0.151 * wind ** 2\r\n","  return h"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YS5ODTuh8YeQ"},"source":["dataset = raw_data.copy()\r\n","dataset2 = drop_clms(dataset)\r\n","dataset2.drop(['Hour', 'Minute'], axis =1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jaX2IszQCSh"},"source":["dataset2['season'] = dataset2.apply(lambda x: 절기24(x['Date']), axis = 1)\r\n","dataset3 = cos_time(dataset2)\r\n","dataset4 = dataset3[['DHI', 'DNI', 'WS', 'RH', 'T', 'TARGET', 'SH', 'sin_time', 'cos_time','season', 'Time']]\r\n","# dataset5 = dataset4.iloc[n_days:,:-1]\r\n","# goals = pd.DataFrame(dataset4.Goal, index = dataset4.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9Ptp410hfqY"},"source":["dataset4.rename(columns = {'T' : 'Temp'},inplace = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yOX-STzBocM"},"source":["dataset5= dataset4.copy()\r\n","\r\n","dataset5['WST'] = dataset5.apply(lambda x: wind_and_hot(x.WS, x.Temp), axis=1)\r\n","dataset5['GHI'] = dataset5.apply(lambda x: HRA(x.DHI, x.DNI, x.season, x.Time), axis=1)\r\n","dataset5.iloc[50:51,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RpNjhWMrND8z"},"source":["dataset5.iloc[50:51,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsFYo0a1NtJ9"},"source":["#dataset6 = dataset5[['DHI', 'DNI', 'WS', 'RH', 'T','SH', 'sin_time', 'cos_time', 'GHI', 'season', 'TARGET']]\r\n","dataset6 = dataset5[['RH', 'Temp', 'WST', 'SH', 'WS', 'GHI', 'sin_time', 'cos_time', 'TARGET']] \r\n","dataset6.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yke9kBn02Q__"},"source":["scaler = MinMaxScaler()\r\n","# scaler = StandardScaler()\r\n","scaler.fit(dataset6)\r\n","temp_X = pd.DataFrame(scaler.transform(dataset6), columns = dataset6.columns)\r\n","temp_y = pd.DataFrame(dataset6['TARGET'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qfzLMGBy_YUJ"},"source":["## dataset6 가지고 동일하게 to_supervised 함수를 써서 나누면 됩니다.\r\n","### 나머지 변수 합치기, 인코딩은 유진이가 해줄 것..."]},{"cell_type":"code","metadata":{"id":"8zMKRkOh86bm"},"source":["dataset7 = train_to_supervised(temp_X,temp_y, n_days)\r\n","dataset7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TIzzIDzeDTQR"},"source":["X = dataset7.iloc[:,:-2]\r\n","#X = encoding(X)\r\n","#X.drop('season', axis =1, inplace=True)\r\n","\r\n","y_1 = pd.DataFrame(dataset7.iloc[:,-2])\r\n","y_2 = pd.DataFrame(dataset7.iloc[:,-1])\r\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mse-YpNROEty"},"source":["# testset은 이미 24절기(season)가 들어있으므로, 약간 다르게 전처리를 해 줘야 함.\r\n","## (season) 구하는 과정이 생략됨.\r\n","## 이하는 testset을 구하는 코드"]},{"cell_type":"code","metadata":{"id":"Im2u9TTwSy7M"},"source":["df_test = []\r\n","\r\n","# 예시 코드\r\n","# 아직 유진이 어떻게 변수 추가할 지 몰라서 to_supervised는 쓰지 않았음.\r\n","\r\n","for i in range(81):\r\n","    file_path = '/content/drive/MyDrive/Jupyter/unlimited_power/raw_data/test/' + str(i) + '.csv'\r\n","    new_path = '/content/drive/MyDrive/Jupyter/unlimited_power/raw_data/test/adj_test/' + str(i) + '.csv'\r\n","    file_name = str(i) + '.csv'\r\n","    \r\n","    # adj_test에서 가져와야 하므로 new_path에서 불러온다\r\n","    temp = pd.read_csv(new_path)\r\n","    temp.drop('Unnamed: 0', axis = 1, inplace = True)\r\n","    testset = temp.copy()\r\n","\r\n","    # 24로 저장한 24절기 season으로 이름 바꾸기\r\n","    testset.rename(columns = {'24' : 'season'}, inplace = True)\r\n","\r\n","    # 이 다음은 trainset과 동일한 전처리, season은 이미 구해져 있으므로 구하지 않음\r\n","    testset2 = drop_clms(testset)\r\n","    testset2.drop(['Hour', 'Minute','Date'], axis=1,inplace=True)\r\n","    testset3 = cos_time(testset2)\r\n","    testset4 = testset3[['DHI', 'DNI', 'WS', 'RH', 'T', 'TARGET', 'SH', 'sin_time', 'cos_time','season', 'Time']]\r\n","    testset4.rename(columns = {'T' : 'Temp'},inplace = True)\r\n","\r\n","    testset5 = testset4.copy()\r\n","    testset5['GHI'] = testset4.apply(lambda x: HRA(x.DHI, x.DNI, x.season, x.Time), axis=1)\r\n","    testset5['WST'] = testset4.apply(lambda x: wind_and_hot(x.WS, x.Temp), axis=1)\r\n","    testset6 = testset5[['RH', 'Temp', 'WST', 'SH', 'WS', 'GHI', 'sin_time', 'cos_time', 'TARGET']] #'DHI', 'DNI', 'season', 'WS', 'SH'\r\n","    testset6 = pd.DataFrame(scaler.transform(testset6), columns=testset6.columns)\r\n","    testset7 = test_to_supervised(testset6, n_days)\r\n","\r\n","#    testset8 = encoding(testset7)\r\n","#    testset8.drop('season', axis = 1, inplace = True)\r\n","    testset9 = testset7.iloc[-48:,:]\r\n","\r\n","    df_test.append(testset9)\r\n","\r\n","X_test = pd.concat(df_test)\r\n","# X_test = X_test.iloc[:, :n_obs]\r\n","X_test "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QivVaEkGK-gR"},"source":["X_train_1, X_valid_1, Y_train_1,  Y_valid_1 = train_test_split(X, y_1, test_size=0.30, random_state=42)\r\n","X_train_2, X_valid_2, Y_train_2,  Y_valid_2 = train_test_split(X, y_2, test_size=0.30, random_state=42)\r\n","\r\n","k=0\r\n","quantiles = [0.1-k, 0.2-k, 0.3-k, 0.4-k, 0.5, 0.6-k, 0.7-k, 0.8-k, 0.9-k]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXHSs1hz8Tf5"},"source":["params = {}\r\n","params['objective'] = 'quantile'\r\n","params['min_child_samples'] = 10\r\n","params['min_child_weight'] = 150\r\n","params['min_split_gain'] = 0\r\n","params['bagging_fraction'] = 0.9\r\n","params['feature_fraction'] = 0.9\r\n","params['learning_rate'] = 0.007\r\n","params['save_binary'] = True\r\n","params['is_unbalance'] = False\r\n","params['subsample'] = 0.9\r\n","params['max_drop'] = 50\r\n","params['max_bin'] = 256\r\n","params['drop_rate'] = 0.1\r\n","params['num_leaves'] = 40\r\n","params['device'] = 'gpu'\r\n","params['max_depth'] = 25\r\n","params['boosting_type'] = \"gbdt\"\r\n","params['Min_data_in_leaf'] = 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DvO2slcLfiH"},"source":["# LGBM 모델을 구축하고 예측까지 하는 함수를 만든다. \r\n","#def LGBM(q, X, Y, X_test):\r\n","\r\n","def LGBM(q, X_train, X_valid, Y_train, Y_valid, X_test, estimators):    \r\n","    # (a) 모델링\r\n","\r\n","    params['n_estimators'] = estimators\r\n","    model = LGBMRegressor(**params, alpha=q, random_state = 42)          \r\n","    \r\n"," #   splits = 3\r\n"," #   kfold = KFold(n_splits = splits, shuffle = True, random_state = 42)\r\n"," #   for n_fold, (trn_idx, val_idx) in enumerate(kfold.split(X)):\r\n"," #     X_train, Y_train = X.iloc[trn_idx], Y.iloc[trn_idx]\r\n"," #     X_valid, Y_valid = X.iloc[val_idx], Y.iloc[val_idx]\r\n"," \r\n","#    rnd_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=100, cv=4, scoring='neg_mean_squared_error', random_state=42)\r\n","    model.fit(X_train, Y_train, eval_metric = ['quantile'], eval_set=[(X_valid, Y_valid)], early_stopping_rounds=500, verbose=500)\r\n","\r\n","\r\n","    # (b) 예측\r\n","    pred = pd.Series(model.predict(X_test).round(2))\r\n","    return pred, model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_GYrgqYqLrO1"},"source":["# Target 예측\r\n","\r\n","def train_data(X_train, Y_train, X_valid, Y_valid, X_test):\r\n","\r\n","#### Feature_selection 모델에 사용할 estimator 개수 ####\r\n","    fs_estimator = 2000\r\n","#### 실제 학습 모델에 사용할 n_estimator 개수 ####\r\n","    estimator = 30000\r\n","\r\n","\r\n","    LGBM_models=[]\r\n","    LGBM_actual_pred = pd.DataFrame()\r\n","\r\n","    for q in quantiles:\r\n","        print(q,'  ########################################### 요인 선정 중 ##############################################')\r\n","        # 요인 선정은 오래 돌릴 필요 없으니까... 1000만 주었음.\r\n","        temp_p, temp_m = LGBM(q, X_train, X_valid, Y_train, Y_valid, X_test, fs_estimator)\r\n","        selected = feature_selection(temp_m, X_train)\r\n","\r\n","        print(q, '  ###################################### {0}개 중 {1}개 선정 완료 ##############################################'.format(len(X_train.columns), len(selected)))\r\n","        pred, model = LGBM(q, X_train[selected], X_valid[selected], Y_train, Y_valid, X_test[selected], estimator)\r\n","        LGBM_models.append(model)\r\n","        LGBM_actual_pred = pd.concat([LGBM_actual_pred,pred],axis=1)\r\n","\r\n","    LGBM_actual_pred.columns=quantiles\r\n","    \r\n","    return LGBM_models, LGBM_actual_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mt5N_at9iWyN"},"source":["# 중요도를 %로 전환하여 반환.\r\n","# 정확도의 총 합이 \"accuracy\" 되는 feature들 내림차순으로 반환\r\n","# 지금은 97 이므로, 97%의 중요도 변수들은 반환하고, 3%를 차지하는 변수들은 중요하지 않다고 판단하여 버린다.\r\n","\r\n","# corrs 는 어느정도 상관계수 이상을 버릴 것인지 정의하는 인자\r\n","# 여기서는 일단 0.95로 설정\r\n","\r\n","accuracy = 97\r\n","corrs = 0.95\r\n","\r\n","def feature_selection(models, input_data):\r\n","  data = pd.DataFrame(models.feature_importances_, index = input_data.columns, columns= ['Importance'])\r\n","  temp_sum = sum(data.Importance)\r\n","  data['Percentage'] = (data.Importance/temp_sum) * 100\r\n","  data = data.sort_values(by = 'Percentage', ascending=False)\r\n","\r\n","  total_imp = 0\r\n","  for i in range(data.shape[0]):\r\n","    imp = data.iloc[i,-1]\r\n","    total_imp += imp\r\n","    if total_imp >= accuracy:\r\n","      break\r\n","  selected_list = list(data.index[:i])\r\n","  selected_data = input_data[selected_list]\r\n","\r\n","##### 이 위 까지는 97%의 정확도를 가지는 특징을 잡아주는 함수, 아래는 상관계수(corr)가 0.95 이상인 변수를 제외하는 함수\r\n","  corr_matrix = selected_data.corr().abs()\r\n","  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\r\n","  high_cor = [column for column in upper.columns if any(upper[column] > corrs)]\r\n","  features = [i for i in selected_data.columns if i not in high_cor]\r\n","\r\n","  return features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1kDBEXW8Ltfi"},"source":["tick = time.time()\r\n","\r\n","# Target1\r\n","models_1, results_1 = train_data(X_train_1, Y_train_1, X_valid_1, Y_valid_1, X_test)\r\n","\r\n","#models_1, results_1 = train_data(X, y_1, X_test)\r\n","results_1.sort_index()[:48]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qlvhd5SwLu0-"},"source":["# Target2\r\n","models_2, results_2 = train_data(X_train_2, Y_train_2, X_valid_2, Y_valid_2, X_test)\r\n","\r\n","#models_2, results_2 = train_data(X, y_2, X_test)\r\n","tock = time.time()\r\n","results_2.sort_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-hFzqsOLwh9"},"source":["print('소요시간 : ', (tock - tick)/60, '분')\r\n","print(results_1.shape, results_2.shape)\r\n","#print('점수 : ', rnd_search.best_score_)\r\n","#print('최적 파라미터 : ', rnd_search.best_params_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JgATlsFLlqHx"},"source":["from scipy.optimize import curve_fit\r\n","import pylab\r\n","import math\r\n","\r\n","# columns_IDA는 상하로 맞춰줌\r\n","# rows_IDA는 좌우로 맞춰줌\r\n","# 상하 > 좌우 & 좌우 > 상하를 비교한 결과 좌우로 먼저 맞춘 다음에 상하를 맞추는 것이 쬐------끔 좋은 결과를 보임.\r\n","\r\n","# 그래서 rows_IDA 먼저 하고 columns_IDA 추천\r\n","\r\n","\r\n","def columns_IDA(dataset):\r\n","  dataset2 = pd.DataFrame()\r\n","  for j in range(0,9):\r\n","    new_val = []\r\n","    for m in range(1, int(raw_data.shape[0]/48)+1):\r\n","      temp = dataset.iloc[48*(m-1):48*m,j].values\r\n","\r\n","      strt = 0\r\n","      endd = 0\r\n","      k = 0\r\n","\r\n","      for i in range(len(temp)):\r\n","        if temp[i] > k and k ==0 and i < 24:\r\n","          strt = i\r\n","        elif temp[i] < k and temp[i] == 0 and i > 24:\r\n","          endd = i\r\n","        k = temp[i]\r\n","      if strt != 0:\r\n","        bf = temp[:strt+1]\r\n","        y = temp[strt+1:endd-1]\r\n","        af = temp[endd-1:]\r\n","        # bf = temp[:strt]\r\n","        # y = temp[strt:endd]\r\n","        # af = temp[endd:]\r\n","\r\n","        x = np.array([i for i in range(len(y))])\r\n","        fit_t = np.polyfit(x,y,5)\r\n","\r\n","        for i in x:\r\n","          fit1 = fit_t[0]*x**5 + fit_t[1]*x**4 + fit_t[2]*x**3 + fit_t[3]*x**2 + fit_t[4]*x + fit_t[5]\r\n","      # fit1 = fit_t[0]*x**4 + fit_t[1]*x**3 + fit_t[2]*x**2 + fit_t[3]*x + fit_t[4]\r\n","      # fit1 = fit_t[0]*x**2 + fit_t[1]*x + fit_t[2]\r\n","\r\n","        revised = np.concatenate((bf, fit1, af), axis=0)\r\n","      elif strt == 0:\r\n","        revised = temp\r\n","      new_val.extend(list(revised))\r\n","    k = pd.DataFrame(new_val)\r\n","    dataset2 = pd.concat([dataset2, k], axis = 1)\r\n","  return dataset2\r\n","\r\n","def f(x, a,b,c, d):\r\n","  b = b**2\r\n","  y = 1/3 * b * x**3 - a*b *x**2 + (a**2*b + b*c**2)*x + d\r\n","  # y = a*(x-12*a) * x **2 + c\r\n","  # y = a*x**2 + b\r\n","  return y\r\n","\r\n","def rows_IDA(dataset):\r\n","  revised = pd.DataFrame()\r\n","  for i in range(dataset.shape[0]):\r\n","    y = dataset.iloc[i,:].values\r\n","    x = np.array([i for i in range(1, 10)])\r\n","\r\n","    if list(y).count(0) <= 6 :\r\n","      popt, pcov = curve_fit(f, x, y, maxfev = 999990000)\r\n","      yfit = f(x, *popt)\r\n","      revised = pd.concat([revised, pd.DataFrame(yfit).transpose()], axis = 0)\r\n","    else:\r\n","      revised = pd.concat([revised, pd.DataFrame(y).transpose()], axis = 0)\r\n","    if i % 1000 == 0:\r\n","      print(i, '/7776')\r\n","  return revised"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpyzax_JLydx"},"source":["# 출력\r\n","\r\n","submission.loc[submission.id.str.contains(\"Day7\"), \"q_0.1\":] = results_1.sort_index().values\r\n","submission.loc[submission.id.str.contains(\"Day8\"), \"q_0.1\":] = results_2.sort_index().values\r\n","submission2 = submission.set_index('id')\r\n","submission3 = not_minus(submission2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"49A4Ztczlt3w"},"source":["dataset2 = rows_IDA(submission3)\r\n","dataset3 = pd.DataFrame(dataset2.values, columns = submission3.columns, index = submission3.index)\r\n","dataset3 = not_minus(dataset3)\r\n","dataset4 = columns_IDA(dataset3)\r\n","dataset5 = pd.DataFrame(dataset4.values, columns = submission3.columns, index = submission3.index)\r\n","submission3 = not_minus(dataset5)\r\n","dataset2 = rows_IDA(submission3)\r\n","dataset3 = pd.DataFrame(dataset2.values, columns = submission3.columns, index = submission3.index)\r\n","submission3 = not_minus(dataset3)\r\n","submission3 = not_dec(submission3)\r\n","submission3 = pd.DataFrame(submission3.values, columns = submission2.columns, index = submission2.index)\r\n","submission3 = not_dec(submission3)\r\n","submission3 = pd.DataFrame(submission3.values, columns = submission2.columns, index = submission2.index)\r\n","submission3 = not_dec(submission3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_yZwPJsUtKc"},"source":["submission4 = pd.DataFrame(submission3.values, columns = submission2.columns, index = submission2.index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9YEmJ9IvLy9K"},"source":["submission4.to_csv('Feature_selection.csv')\r\n","!cp Feature_selection.csv \"drive/My Drive/\""],"execution_count":null,"outputs":[]}]}