{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GB_jy1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/xbeafraid1/ILTAL/blob/main/Python_GBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"-Qqer4Kk5cyt"},"source":["#지금 만들어야 하는 모델 : \r\n","CatBoost, AdaBoost, XGBoost, LGBM 중 어느 것을 쓸 지 아직 정해지지 않았지만 공통적으로 적용될 수 있는 모델을 만들어보려고 함.\r\n","\r\n","\r\n","\r\n","### 5일, 4일, 3일, 2일, 1일 중 어느 것으로 앞의 것들을 예측해볼 지는 아직 정해지지 않음.\r\n"]},{"cell_type":"code","metadata":{"id":"8y4Y__WDtwtR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609132935001,"user_tz":-540,"elapsed":5251,"user":{"displayName":"JAE YOON CHO","photoUrl":"","userId":"01784735957019496550"}},"outputId":"4a62164c-e02c-4f50-f3f6-6786a1417151"},"source":["import pandas as pd\r\n","import numpy as np\r\n","import seaborn as sns\r\n","import matplotlib.pyplot as plt\r\n","import itertools\r\n","import time\r\n","import torch\r\n","import statsmodels.api as sm\r\n","from sklearn import linear_model, metrics\r\n","from sklearn.model_selection import train_test_split\r\n","from tqdm import tnrange, tqdm_notebook\r\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\r\n","from statsmodels.regression.quantile_regression import QuantReg\r\n","from sklearn.ensemble import GradientBoostingRegressor\r\n","from xgboost import XGBRegressor\r\n","from pandas import DataFrame, concat\r\n","import torch\r\n","import os"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaN22hIb2P7d","executionInfo":{"status":"ok","timestamp":1609132965248,"user_tz":-540,"elapsed":24969,"user":{"displayName":"JAE YOON CHO","photoUrl":"","userId":"01784735957019496550"}},"outputId":"ca07bf86-4a3c-4580-88e5-f05cdd9c0b5c"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gzTSsEIU0Oq0"},"source":["raw_data = pd.read_csv('/content/drive/MyDrive/Jupyter/unlimited_power/raw_data/train/train.csv') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WaBO9r1W737g"},"source":["# !cp를 이용하여 기존에 있었던 함수를 불러온다.\r\n","# GB_functions.ipynb에 전처리 함수를 저장해 놓았으므로 얘를 JY로 불러온다.\r\n","!cp /content/drive/MyDrive/Jupyter/unlimited_power/working__/GB_functions.ipynb .\r\n","import GB_functions as JY\r\n","\r\n","# 불러온 데이터를 GB_functions에 미리 정의해 둔 data_loadder 함수에 때려 넣으면\r\n","# 자동으로 train_X, train_y, test_X, test_y 4가지가 선언된다.\r\n","train_set, test_set = MJ.preprocessing(raw_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"24w0rHQb2ZdE"},"source":["# Hour와 Minute 변수를 활용하여 Time 변수 추가함.\r\n","\r\n","def drop_clms(dataset):\r\n","  dataset['Time'] = dataset['Hour'] + dataset['Minute']*(0.5/30)\r\n","  dataset = dataset[what_to_use]\r\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"46Lt7Zao9Eno"},"source":["# 머신러닝에 쓰기 위해서 재정렬 시키는 함수 (series 함수를 supervised 형태로 바꾸어주는 함수) - Multivariable을 이용한 Gradient Boost 모델에서도 필요함.\r\n","def series_to_supervised(data, n_in=1, n_out=1, target = 'TARGET', dropnan=True):\r\n","    df = DataFrame(data)\r\n","    df.drop(target, axis = 1, inplace=True)\r\n","    df2 = DataFrame(data[target])\r\n","    cols, names = list(), list()\r\n","    n_vars = 1 if type(df) is list else df.shape[1]\r\n","    n_vars2 = 1 if type(df2) is list else df2.shape[1]\r\n","    # input sequence (t-n, ... t-1)\r\n","    for i in range(n_in, 0, -1):\r\n","        cols.append(df.shift(i))\r\n","        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\r\n","    # forecast sequence (t, t+1, ... t+n)\r\n","    for i in range(0, n_out):\r\n","        cols.append(df2.shift(-i))\r\n","        if i == 0:\r\n","            names += [('TARGET%d(t)' % (j+1)) for j in range(n_vars2)]\r\n","        else:\r\n","            names += [('TARGET%d(t+%d)' % (j+1, i)) for j in range(n_vars2)]\r\n","    # put it all together\r\n","    agg = concat(cols, axis=1)\r\n","    agg.columns = names\r\n","    # drop rows with NaN values\r\n","    if dropnan:\r\n","        agg.dropna(inplace=True)\r\n","    return agg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5Xk7adGxorQ"},"source":["def data_maker(train_X, train_y, test_X, test_y):\r\n","  train_X = torch.tensor(np.array(train_X), dtype=torch.float)\r\n","  train_X = train_X.transpose(1,0)\r\n","  train_y = torch.tensor(np.array(train_y), dtype=torch.float)\r\n","\r\n","  test_X = torch.tensor(np.array(test_X), dtype=torch.float)\r\n","  test_X = test_X.transpose(1,0)\r\n","  test_y = torch.tensor(np.array(test_y), dtype=torch.float)\r\n","  return train_X, train_y, test_X, test_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DCHxP8PvJpLr"},"source":["def data_loader(dataset):\r\n","\r\n","  ############ 파라미터 #####################\r\n","  # 하루의 틱\r\n","  ticks = 48\r\n","  # 예측에 사용할 일수\r\n","  days = 7\r\n","  n_days = ticks*days\r\n","\r\n","  # 미래 예측할 일수\r\n","  future_days = 2\r\n","  future_window = ticks * future_days\r\n","\r\n","  ### 모든변수\r\n","  # ['Hour', 'Minute', 'Day', 'WS', 'Time', 'DHI','DNI','RH','T','TARGET']\r\n","  # 사용할 변수\r\n","  what_to_use = ['Time', 'DHI','DNI','RH','T','TARGET']\r\n","\r\n","  n_features = len(what_to_use)\r\n","  n_obs = n_days * n_features\r\n","\r\n","\r\n","  dataset = drop_clms(dataset)\r\n","  dataset = series_to_supervised(dataset, n_days, future_window, target='TARGET')\r\n","  train_X, train_y, test_X, test_y = seperator(dataset)\r\n","  train_X, train_y, test_X, test_y = data_maker(train_X, train_y, test_X, test_y)\r\n","  return train_X, train_y, test_X, test_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccVAfcvlLNIL"},"source":["# 시계열 분석의 백테스팅 - walk-forward validation for univariate data\r\n","# Cut Point를 정해서 그 전은 Train / 그 이후는 Test Set으로 정함.\r\n","# 하나의 Step을 정한 다음에, 예를 들어서 1 week forecast라고 하면 test의 데이터로 1 step 예측 후, 1 step 간 부분의 real data 까지 포함해서 그 다음 부분을 예측하는 방식.\r\n","def walk_forward_validation(data, n_test):\r\n","\tpredictions = list()\r\n","\t# 데이터셋 분리\r\n","\ttrain, test = train_test_split(data, n_test)\r\n","\t# train 데이터셋의 seed 히스토리\r\n","\thistory = [x for x in train]\r\n","\t# test set을 부분부분 train set에 포함시킴.\r\n","\tfor i in range(len(test)):\r\n","\t\t# split test row into input and output columns\r\n","\t\ttestX, testy = test[i, :-1], test[i, -1]\r\n","\t\t# fit model on history and make a prediction - One Step Forecast 수행함.\r\n","\t\tyhat = xgboost_forecast(history, testX)\r\n","\t\t# store forecast in list of predictions\r\n","\t\tpredictions.append(yhat)\r\n","\t\t# add actual observation to history for the next loop - 여기서 actual observation을 history에 추가시킴.\r\n","\t\thistory.append(test[i])\r\n","\t\t# summarize progress\r\n","\t\tprint('>expected=%.1f, predicted=%.1f' % (testy, yhat))\r\n","\t# estimate prediction error\r\n","\terror = mean_absolute_error(test[:, -1], predictions)\r\n","\treturn error, test[:, 1], predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K72B7jZyMjOC"},"source":["# train과 test set으로 분리하는 train_test_split\r\n","def train_test_split(data, n_test):\r\n","\treturn data[:-n_test, :], data[-n_test:, :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkiUCkRtq9Bb"},"source":["#  XGBRegressor 사용하여 One-step 예측하는 함수를 구축함.\r\n","\r\n","def xgboost_forecast(train, testX):\r\n","\t# transform list into array\r\n","\ttrain = asarray(train)\r\n","\t# split into input and output columns\r\n","\ttrainX, trainy = train[:, :-1], train[:, -1]\r\n","\t# fit model\r\n","\tmodel = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\r\n","\tmodel.fit(trainX, trainy)\r\n","\t# make a one-step prediction\r\n","\tyhat = model.predict([testX])\r\n","\treturn yhat[0]\r\n","\r\n","\r\n","  ## 여기서 더 못나가겠다 무슨 말하는지 모르겠다 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLP9zT551YuE"},"source":["############ 파라미터 #####################\r\n","# 하루의 틱\r\n","ticks = 48\r\n","# 예측에 사용할 일수\r\n","days = 7\r\n","n_days = ticks*days\r\n","\r\n","# 미래 예측할 일수\r\n","future_days = 2\r\n","future_window = ticks * future_days\r\n","\r\n","### 모든변수\r\n","# ['Hour', 'Minute', 'Day', 'WS', 'Time', 'DHI','DNI','RH','T','TARGET']\r\n","# 사용할 변수\r\n","what_to_use = ['Time', 'DHI','DNI','RH','T','TARGET']\r\n","\r\n","n_features = len(what_to_use) - 1\r\n","n_obs = n_days * n_features\r\n","\r\n","# 한 번에 뭉테기로 투입할 자료의 양\r\n","batches = 100\r\n","# 몇 번이나 반복하여 학습할 것인다.\r\n","epoch = 200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_Qds7Qbraqk"},"source":["train_X, train_y, test_X, test_y = data_loader(raw_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zvl4yY5F6ClD","outputId":"f59a7a62-febf-40bc-9fad-1cbdbcedd827"},"source":["type(train_X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Tensor"]},"metadata":{"tags":[]},"execution_count":45}]}]}