{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM_mk3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1KvPuJzzKBIetEhzRCFkKxUv-mxtjR3V8","authorship_tag":"ABX9TyMc7aujpS2bBQycpf40NQ9A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gH7tceTn_JfT"},"source":["점점 미쳐가는 중\r\n","\r\n","###사유\r\n","\r\n","7일의 데이터를 사용해서 2일의 데이터를 예측해야한다ㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏㅏ\r\n","\r\n","\r\n","###문제\r\n","\r\n","많은 것이 문제, 무엇보다 문제는 **input과 output의 모양이 달라서** 도통 되먹지를 않는다.\r\n","\r\n","일단 7일치를 2일치로 퉁치는 것이 문제임\r\n","\r\n","두 번째 문제는 변수 6개로 1개 예측하는 것이 문제임\r\n","\r\n","합치면, **7일치의 변수 6개로, 2일치 정답변수 1개를 예측하는 문제**가 되어서 난장판ㄴㄴㄴㄴㄴ\r\n","\r\n","문제2, 원래 LSTM은 이미지 처리, 텍스트 처리에 쓰는 경우가 많아서 일단 이런 회귀는 다른 모델이 더 좋다. 애초에 들어가서 구동이 안됨\r\n","\r\n","\r\n","### 방안\r\n","\r\n","자연어(NLP) 해석 신경망을 훔쳐왔다.\r\n","\r\n","**한국어 - 영어 해석은 input과 output shape(글자의 숫자, 주어/동사/목적어)이 달라도 돌아간다.**\r\n","\r\n","그런고로 7일치 6개의 변수 > 2일치 1개의 정답도 대충 되지 않을까 기대함.\r\n","\r\n","안되면 환불손절"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4QgGhECmF2Rl","executionInfo":{"status":"ok","timestamp":1609005473051,"user_tz":-540,"elapsed":5589,"user":{"displayName":"‍최명진(학부학생/사회과학대학 사회복지학과)","photoUrl":"","userId":"07688453603143010105"}},"outputId":"d7d3e76c-6713-4d18-9be2-204d491a1dea"},"source":["from google.colab import drive\r\n","\r\n","\r\n","drive.mount('/content/drive')\r\n","!cp /content/drive/MyDrive/Jupyter/unlimited_power/working__/lstm_defs.py .\r\n","import lstm_defs as MJ"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bNzfUuVt-uFM","executionInfo":{"status":"ok","timestamp":1609005420581,"user_tz":-540,"elapsed":992,"user":{"displayName":"‍최명진(학부학생/사회과학대학 사회복지학과)","photoUrl":"","userId":"07688453603143010105"}}},"source":["import pandas as pd\r\n","from pandas import Series, DataFrame\r\n","from matplotlib import pyplot\r\n","import matplotlib.pyplot as plt\r\n","from sklearn.preprocessing import MinMaxScaler\r\n","import numpy as np\r\n","import seaborn as sns\r\n","from pandas import DataFrame\r\n","from pandas import concat\r\n","import os\r\n","from sklearn.model_selection import train_test_split\r\n","import torch\r\n","import torch.nn as nn\r\n","from torch.utils.data import TensorDataset, DataLoader\r\n","import torch.optim as optim"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wZeTugBGKf5","executionInfo":{"status":"aborted","timestamp":1609005167693,"user_tz":-540,"elapsed":24669,"user":{"displayName":"‍최명진(학부학생/사회과학대학 사회복지학과)","photoUrl":"","userId":"07688453603143010105"}}},"source":["raw_data = pd.read_csv('/content/drive/MyDrive/Jupyter/unlimited_power/raw_data/train/train.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6BJ8W7-qGgrg","executionInfo":{"status":"aborted","timestamp":1609005167693,"user_tz":-540,"elapsed":24667,"user":{"displayName":"‍최명진(학부학생/사회과학대학 사회복지학과)","photoUrl":"","userId":"07688453603143010105"}}},"source":["# 불러온 데이터를 MJ.py에 미리 정의해 둔 data_loadder 함수에 때려 넣으면\r\n","# 자동으로 train_X, train_y, test_X, test_y 4가지가 선언된다.\r\n","train_X, train_y, test_X, test_y = MJ.data_loader(raw_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHSufBeM-0Zy","executionInfo":{"status":"aborted","timestamp":1609005167693,"user_tz":-540,"elapsed":24665,"user":{"displayName":"‍최명진(학부학생/사회과학대학 사회복지학과)","photoUrl":"","userId":"07688453603143010105"}}},"source":["class Encoder(nn.Module):\r\n","\r\n","    def __init__(self, input_size, hidden_dim, num_layers=1):\r\n","        super(Encoder, self).__init__()\r\n","\r\n","        self.input_size = input_size\r\n","        self.hidden_dim = hidden_dim\r\n","        self.num_layers = num_layers\r\n","        self.lstm = nn.LSTM(self.input_size, self.hidden_dim, num_layers=self.num_layers)\r\n","        self.hidden = None\r\n","\r\n","    def init_hidden(self, batch_size):\r\n","        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\r\n","                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\r\n","\r\n","    def forward(self, inputs):\r\n","        # Push through RNN layer (the ouput is irrelevant)\r\n","        _, self.hidden = self.lstm(inputs, self.hidden)\r\n","        return self.hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFS4Dkbn-4zb","executionInfo":{"status":"aborted","timestamp":1609005167694,"user_tz":-540,"elapsed":24664,"user":{"displayName":"‍최명진(학부학생/사회과학대학 사회복지학과)","photoUrl":"","userId":"07688453603143010105"}}},"source":["class Decoder(nn.Module):\r\n","\r\n","    def __init__(self, hidden_dim, num_layers=1):\r\n","        super(Decoder, self).__init__()\r\n","        # input_size=1 since the output are single values\r\n","        self.lstm = nn.LSTM(1, hidden_dim, num_layers=num_layers)\r\n","        self.out = nn.Linear(hidden_dim, 1)\r\n","\r\n","    def forward(self, outputs, hidden, criterion):\r\n","        batch_size, num_steps = outputs.shape\r\n","        # Create initial start value/token\r\n","        input = torch.tensor([[0.0]] * batch_size, dtype=torch.float)\r\n","        # Convert (batch_size, output_size) to (seq_len, batch_size, output_size)\r\n","        input = input.unsqueeze(0)\r\n","\r\n","        loss = 0\r\n","        for i in range(num_steps):\r\n","            # Push current input through LSTM: (seq_len=1, batch_size, input_size=1)\r\n","            output, hidden = self.lstm(input, hidden)\r\n","            # Push the output of last step through linear layer; returns (batch_size, 1)\r\n","            output = self.out(output[-1])\r\n","            # Generate input for next step by adding seq_len dimension (see above)\r\n","            input = output.unsqueeze(0)\r\n","            # Compute loss between predicted value and true value\r\n","            loss += criterion(output, outputs[:, i])\r\n","        return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKVZ_exOAYI2","executionInfo":{"status":"aborted","timestamp":1609005167694,"user_tz":-540,"elapsed":24662,"user":{"displayName":"‍최명진(학부학생/사회과학대학 사회복지학과)","photoUrl":"","userId":"07688453603143010105"}}},"source":["if __name__ == '__main__':\r\n","\r\n","    # 5 is the number of features of your data points\r\n","    encoder = Encoder(5, 16)\r\n","    decoder = Decoder(16)\r\n","    # Create optimizers for encoder and decoder\r\n","    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\r\n","    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\r\n","    criterion = nn.MSELoss()\r\n","\r\n","    \r\n","    # Convert (batch_size, seq_len, input_size) to (seq_len, batch_size, input_size)\r\n","    inputs = train_X\r\n","\r\n","    # 2 sequences (to match the batch size) of length 6 (for the 6h into the future)\r\n","    outputs = train_y\r\n","\r\n","    #\r\n","    # Do one complete forward & backward pass\r\n","    #\r\n","    # Zero gradients of both optimizers\r\n","    encoder_optimizer.zero_grad()\r\n","    decoder_optimizer.zero_grad()\r\n","    # Reset hidden state of encoder for current batch\r\n","    encoder.hidden = encoder.init_hidden(inputs.shape[1])\r\n","    # Do forward pass through encoder\r\n","    hidden = encoder(inputs)\r\n","    # Do forward pass through decoder (decoder gets hidden state from encoder)\r\n","    loss = decoder(outputs, hidden, criterion)\r\n","    # Backpropagation\r\n","    loss.backward()\r\n","    # Update parameters\r\n","    encoder_optimizer.step()\r\n","    decoder_optimizer.step()\r\n","    print(\"Loss:\", loss.item())"],"execution_count":null,"outputs":[]}]}